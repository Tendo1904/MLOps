{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "model = YOLO('best.pt')\n",
    "\n",
    "os.makedirs('runs',exist_ok=True)\n",
    "os.makedirs('export',exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.27  Python-3.10.11 torch-2.0.0+cu117 CPU (AMD Ryzen 5 3500U with Radeon Vega Mobile Gfx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 168 layers, 3,006,233 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\artem\\Some stuff\\MLOps\\Lab2\\data\\face-mask-detection-yolo\\labels\\val.cache... 171 images, 0 backgrounds, 0 corrupt: 100%|██████████| 171/171 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 171/171 [00:17<00:00,  9.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        171        729      0.884      0.683      0.764      0.512\n",
      " mask_weared_incorrect         23         24      0.865      0.625      0.694      0.478\n",
      "             with_mask        151        604      0.941      0.821      0.908      0.616\n",
      "          without_mask         49        101      0.845      0.604      0.689      0.441\n",
      "Speed: 1.3ms preprocess, 92.4ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "Saving runs\\val27\\predictions.json...\n",
      "Results saved to \u001b[1mruns\\val27\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = model.val(data='../Lab2/data/face-mask-detection-yolo/data.yaml', batch=1, project='runs', save_json=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.27  Python-3.10.11 torch-2.0.0+cu117 CPU (AMD Ryzen 5 3500U with Radeon Vega Mobile Gfx)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'best.pt' with input shape (1, 3, 416, 416) BCHW and output shape(s) (1, 7, 3549) (5.9 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 17...\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.39...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  2.3s, saved as 'best.onnx' (11.6 MB)\n",
      "\n",
      "Export complete (2.6s)\n",
      "Results saved to \u001b[1mC:\\Users\\artem\\Some stuff\\MLOps\\Lab3\u001b[0m\n",
      "Predict:         yolo predict task=detect model=best.onnx imgsz=416  \n",
      "Validate:        yolo val task=detect model=best.onnx imgsz=416 data=data/face-mask-detection-yolo/data.yaml  \n",
      "Visualize:       https://netron.app\n",
      "ONNX model simplified and saved.\n"
     ]
    }
   ],
   "source": [
    "model.export(format='onnx', batch=1)\n",
    "\n",
    "from onnxsim import simplify\n",
    "import onnx\n",
    "\n",
    "# Load ONNX model\n",
    "model_path = \"best.onnx\"  \n",
    "onnx_model = onnx.load(model_path)\n",
    "\n",
    "# Simplify the model\n",
    "simplified_model, check = simplify(onnx_model)\n",
    "\n",
    "if check:\n",
    "    onnx.save(simplified_model, \"best_op.onnx\")\n",
    "    print(\"ONNX model simplified and saved.\")\n",
    "else:\n",
    "    print(\"Simplification failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Ultralytics 8.3.27  Python-3.10.11 torch-2.0.0+cu117 CPU (AMD Ryzen 5 3500U with Radeon Vega Mobile Gfx)\n",
      "Loading best_op.onnx for ONNX Runtime inference...\n",
      "Preferring ONNX Runtime AzureExecutionProvider\n",
      "Setting batch=1 input of shape (1, 3, 416, 416)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\artem\\Some stuff\\MLOps\\Lab2\\data\\face-mask-detection-yolo\\labels\\val.cache... 171 images, 0 backgrounds, 0 corrupt: 100%|██████████| 171/171 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 171/171 [00:10<00:00, 15.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        171        729      0.935      0.651      0.751      0.501\n",
      " mask_weared_incorrect         23         24      0.932      0.574      0.661      0.454\n",
      "             with_mask        151        604      0.946      0.806      0.904      0.613\n",
      "          without_mask         49        101      0.928      0.574      0.689      0.438\n",
      "Speed: 1.8ms preprocess, 52.6ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "Saving runs\\val28\\predictions.json...\n",
      "Results saved to \u001b[1mruns\\val28\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "onnx_model = YOLO('best_op.onnx')\n",
    "\n",
    "results = onnx_model.val(data='../Lab2/data/face-mask-detection-yolo/data.yaml', imgsz=416, batch=1, project='runs', save_json=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\Users\\artem\\Some stuff\\MLOps\\Lab3\\test6.jpg: 416x416 1 mask_weared_incorrect, 7 with_masks, 57.3ms\n",
      "Speed: 4.0ms preprocess, 57.3ms inference, 5.0ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "image 1/1 c:\\Users\\artem\\Some stuff\\MLOps\\Lab3\\test6.jpg: 288x416 1 mask_weared_incorrect, 7 with_masks, 96.8ms\n",
      "Speed: 3.0ms preprocess, 96.8ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 416)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = onnx_model.predict(\"test6.jpg\", imgsz=416)\n",
    "#print(res)\n",
    "res_m = model.predict(\"test6.jpg\", imgsz=416)\n",
    "#print(res_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "image_path = \"test6.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "for result in results:\n",
    "    boxes = result.boxes.xyxy  # Bounding boxes (x1, y1, x2, y2)\n",
    "    confs = result.boxes.conf  # Confidence scores\n",
    "    labels = result.boxes.cls  # Class labels\n",
    "\n",
    "    # Draw bounding boxes\n",
    "    for i, box in enumerate(boxes):\n",
    "        x1, y1, x2, y2 = map(int, box.tolist())  # Convert box coordinates to integers\n",
    "        conf = confs[i]\n",
    "        cls = int(labels[i])  # Class index\n",
    "        label = f\"{result.names[cls]} {conf:.2f}\"  # Class name and confidence\n",
    "\n",
    "        # Draw the rectangle\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "\n",
    "        # Add the label\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            label,\n",
    "            (x1, y1 - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.5,\n",
    "            (255, 0, 0),\n",
    "            2,\n",
    "        )\n",
    "\n",
    "# Show the image with bounding boxes\n",
    "cv2.imshow(\"Detections\", cv2.cvtColor(image, cv2.COLOR_RGB2BGR))  # Convert back to BGR for OpenCV display\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
